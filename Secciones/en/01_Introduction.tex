%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   INTRODUCCIÃ“N
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction} \label{section:en/introduccion}

Artificial Intelligence (AI) is a type of technology that has been developed substantially in recent years. With the advent of Deep Learning (DL), there are more studies and newly discovered applications of these models in different areas. Most of AI algorithms are based on black box models, which render the traceability unclear on how the result was obtained. In general terms, it can be said that, the more complex the AI model, the harder it is to interpret its results. 
%This is evident when the algorithms are compared to decision trees and neural networks. 
A clear example of this is neural networks, widely used today.
On the other hand, opposed to black box models, there exist white box models, whose intention is to guarantee a clear and direct understanding of the process \cite{Ye2021}.

It has been proved that the implementation of black box models of AI does yield good results in diverse fields of application \cite{Verma2023}. However, in histopathology, applying them as part of the Computer-Aided Diagnosis (CAD) is still a difficult task, due to specific requirements of this field that need to be met, among which the lack of transparency of the process becomes its main obstacle \cite{Abdelsamea2022}. To counter this, numerous techniques known as Explainable Artificial Intelligence (xAI) have been developed, including post-hoc and ante-hoc models. It consists of a collection of processes and methods which allow users to comprehend how an AI algorithm reached a certain result \cite{Giuste2023}. Through these xAI techniques, it is possible to obtain more qualitative evidence of the predictions made by the AI algorithm.

Today, the number of professional pathologists is not high in contrast with the total population, and, worldwide, the cases of cancer have a tendency to rise each year \cite{Zehra2023}. In the face of this, the implementation of AI could contribute to prevent a work overload on pathologists \cite{Liu2021}. Nevertheless, despite the multiple xAI techniques, these are not enough to implement AI models in histopathology, given the lack of a clear consensus in the area. Some researchers support that the method through which an AI analyzes the behavior of an algorithm does not consist of explications or predictions, but rather determining the presence of biases in obtaining them \cite{Ahmad2018}. There exists, furthermore, a debate surrounding the use of post-hoc xAI models (black box) and ante-hoc xAI models: the explanations obtained by a black box do not align with the real process an AI model uses to predict its results, an aspect that provokes certain skepticism when it comes to incorporating these technologies into high-stakes scenarios \cite{Ahmad2018}. AI/xAI systems must also be consistent with its results: it must be able to reproduce or recognize each result in the same manner.

Some authors also declare that doctors and healthcare professionals seek to bond with the systems as if they were colleagues, consulting with them their medical point of view, their experiences and weaknesses, and how to better complement their abilities \cite{Cai2019}. In addition, the implementation of AI/xAI models in histology is subject to technical, social and legal requirements.

With the intention of providing a wider picture, this article will present a rapid literature review bringing together all modern requirements and obstacles that the implementation of AI/xAI faces in histopathology.

This article is structured as follows: Section \ref{section:en/protocolo} describes the research protocol used for the rapid review. In Section \ref{section:en/resultados}, results obtained are analyzed and an answer is provided to the question that drives this review. Finally, in Section \ref{section:en/conclusiones}, conclusions are presented. 
