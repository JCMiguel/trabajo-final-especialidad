%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   PI1 - ANÁLISIS DE RESULTADOS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Dificultades técnicas}

%In supervised AI systems, it is crucial to rely on datasets whose samples are classified by an expert with the intention of obtaining consistent predictions. However, many authors declare there are operative difficulties when classifying.
En sistemas supervisados de IA es crucial que las muestras de los datasets sean clasificadas por un experto para que las predicciones del sistema sean consistentes. Sin embargo, algunos autores declaran que la clasificación está sujeta a problemas operativos.

%Ayorinde et al. \cite{Ayorinde2022} analyze that a reliable categorization can be problematic for some elements of slide assessment, especially in determination of arteriosclerosis. Arteriosclerosis is recognized by the narrowing of the blood vessel lumen, although this is not always the case: an apparently narrow lumen may arise not only from disease but also from oblique sectioning. Thus, when processing blood vessel slides, the similarity between them becomes a critical issue for AI implementation due to the possibility of arriving at an erroneous diagnosis.
Ayorinde et al. \cite{Ayorinde2022} analiza que disponer de una clasificación confiable puede ser problemático para la evaluación de ciertos elementos en las muestras, principalmente en la determinación de arterioesclerosis. La arterioesclerosis se puede reconocer por el angostamiento del lumen en el vaso sanguíneo, aunque es posible confundir esto con un seccionamiento oblicuo. Es decir, la similitud entre ambos tipos de muestra es un problema crítico para la implementación de IA debido a la probabilidad de arribar a un diagnóstico erróneo.

%Tran et al. \cite{Tran2021} remark that this is also limiting in oncology, and that it is currently unclear how DL systems would deal with this inter- and intra-laboratory variability. They also add that, in many DL systems, the predictions are simply the best guess with the highest probability. In critical circumstances, overconfident predictions, e.g. predicting cancer primary site with only 40\% certainty, can result in inaccurate diagnosis or cancer management decisions.
Tran et al. \cite{Tran2021} destacan que esto también es limitante en oncología dado que no es precisa la forma en que los sistemas de aprendizaje profundo lidiarían con la variabilidad inter e intralaboratorio. Añaden que en muchos de estos sistemas las predicciones son inherentemente suposiciones con una alta probabilidad de ser ciertas. En circunstancias críticas una estimación arriesgada, como determinar cáncer con un 40\% de certeza, puede provocar diagnósticos y tratamientos erróneos.

%Along these lines, Mohammadi et al. \cite{Mohammadi2022}, mention that a fully supervised learning for whole slide image–based diagnostic tasks in histopathology is problematic due to the requirement for costly and time-consuming manual annotation by experts, and propose the use of weakly supervised learning methods in order to reduce costs at scale. When it comes to processing, Mohammadi et al. comment that training AI models on gigapixel size whole-slide images is highly expensive and that they usually resort to patching approaches.
Mohammadi et al. \cite{Mohammadi2022} mencionan que el uso de algoritmos basados en técnicas de aprendizaje fuertemente supervisado es problemático aplicado a tareas de diagnóstico basadas WSI. Esto se debe a que la clasificación de cada corte histológico demanda mucho tiempo y esfuerzo a los expertos. Para atender esta problemática, los autores proponen el uso de métodos de aprendizaje semi-supervisado. En cuanto al procesamiento, mencionan que entrenar un modelo de IA/IAX con imágenes de gigapíxeles de tamaño es muy costoso y suelen recurrir a métodos de parcheo.

%Dos-Santos et al. \cite{dosSantos2022} state that the lack of diversity in datasets is another difficulty that AI systems face. Moreover, the datasets must be in some way linked to clinical patient data to allow the validation of external algorithms, in addition to morphological validations performed by pathologists.
Dos-Santos et al. \cite{dosSantos2022} declaran que la falta de diversidad en los sets de datos es otra problemática subyacente a los sistemas de IA/IAX. Por otro lado, observan que los conjuntos de datos deben estar vinculados de alguna manera a los datos clínicos de los pacientes para permitir la validación de algoritmos externos, además de ciertas validaciones morfológicas realizadas por los patólogos.


%An important aspect to consider is the data privacy of the patients. On this, Rösler et al. \cite{Roesler2023} say that data in medicine must be securely stored, transferred, and protected from unauthorized access.
No obstante, la privacidad de los datos de los pacientes, en casos donde estos sean necesarios, es otro aspecto importante a considerar en la práctica. En este sentido, Rösler et al. \cite{Roesler2023} observan que los datos en medicina deben almacenarse de manera segura y ser transferido y protegidos de accesos no autorizados.

%On the other hand, Holzinger et al. \cite{Holzinger2019}, reflect on the fact that oftentimes the datasets are not “big” enough. They observe that there is an inherent tension between ML performance (predictive accuracy) and explainability. Often the best-performing AI models are the least transparent, and the ones providing a clear explanation are less accurate.
Por otro lado, Holzinger et al. \cite{Holzinger2019} reflexionan sobre el hecho de que algunos sets de datos no son suficientemente abundantes. Existe una relación de compromiso entre el desempeño de los modelos de aprendizaje automático, medida en términos de la exactitud predictiva, y su explicabilidad. Con frecuencia los modelos IA/IAX más performantes son los menos transparentes, mientras que los más explicados son más inexactos.

%% A escribir...
Civit-Masot et al. \cite{CivitMasot2024} comentan que unos pocos sistemas de diagnóstico asistido logran una exactitud del 100\%, y que esto se debe a que generalmente prueban su desempeño con un subconjunto de las muestras que se utilizaron durante el entrenamiento. Esto puede ocasionar errores de clasificación cuando se evalúan muestras tomadas con diferentes dispositivos de escaneo. Añaden, además, que en redes neuronales los pesos y parámetros internos son completamente incomprensibles y no permiten interpretar cómo el algoritmo alcanza un resultado dado.

Zhou et al. \cite{Zhou2021} postulan una serie de problemáticas subyacentes a los datos necesarios para implementar técnicas de aprendizaje profundo. Observan que algunas tareas requieren diferentes etiquetados de imágenes médicas, lo que es costoso en términos de tiempo, y que la diferencia de experiencia entre el personal ocasiona disparidad en el detalle de las etiquetas. Las muestras, por su parte, presentan disparidad entre sí, dado que los hospitales no suelen utilizar el mismo equipamiento ni configuración para obtenerlas. Por otro lado, la ocurrencia de enfermedades se rige por una distribución sesgada de cola larga o cola de Pareto. Esto significa que, aunque se dispone de un gran volumen de datos para un número pequeño de enfermedades, la mayoría de las enfermedades son poco frecuentes en clínicas. Dado que la inteligencia artificial requiere un volumen elevado de datos confiables, estos factores pueden inducir sesgos en las predicciones de un modelo.

Respecto a la clasificación de imágenes de biopsia, Guleria et al \cite{Guleria2021} observan que el reconocimiento basado en aprendizaje profundo requiere una basta cantidad de muestras etiquetadas que le permiten aprender a detectar las características anormales de los tejidos. Esta tarea de etiquetado suele ser tediosa y sostienen que es un obstáculo que dificulta a los investigadores desarrollar sus modelos de IA.

Por otra parte, Tjoa y Guan \cite{Tjoa&Guan2021} destacan que las verdades fundamentales, ground truths como las llaman en inglés, provistas por los profesionales de áreas médicas no siempre son completamente correctas. Esto puede desencadenar predicciones erradas a causa de los datos incorrectos de aprendizaje.

Finzel et al. \cite{Finzel2022} comentan las ventajas del uso de redes neuronales gráficas (GNN) en aplicaciones médicas y biológicas, pero destacan que, a pesar de que simplifican la necesidad de seleccionar características o definir funciones matemáticas específicas para ciertas distribuciones de datos, aún es difícil para un usuario interpretar por qué una GNN arriba a un determinado resultado. Debido a esto se han estado investigando técnicas para aumentar su interpretabilidad, ya sea usando estrategias generativas, backpropagation, métodos basados en perturbación, entre otros. No obstante, los autores observan que existe la necesidad de evaluar la validez conceptual de una GNN.

Nazar et al. \cite{Nazar2021} en su revisión sistemática exponen una tabla con los desafíos que afronta la inteligencia artificial explicable. A nivel técnico, destacan la dificultad en aportar interpretabilidad a los algoritmos de aprendizaje profundo, que son de caja negra, y problemas relacionados al desempeño del sistema. Los autores observan que el rendimiento del algoritmo IAX condiciona el desempeño total del sistema. De esto se desprende la necesidad de obtener la mejor técnica de explicabilidad posible para aumentar la transparencia del modelo.

Palatnik de Sousa et al \cite{PalatnikdeSousa2019} encuentran ventajas en utilizar LIME como técnica de explicabilidad, dado que al ser agnóstico del modelo podría ser más fácil comparar resultados con estudios futuros, lo cual no ocurre con técnicas de gradiente o saliencia. Advierten, además, que los mapas de calor de saliencia han demostrado explicaciones no confiables en ciertas condiciones.

Gu et al. \cite{Gu2021Lessons} advierten que la mayor dificultad para la patología digital radica en que los datos histológicos tienden a tener una alta varianza entre pacientes. Por lo tanto, un modelo pre-entrenado a menudo tiene dificultades para generalizar resultados cuando se aplica con un nuevo conjunto de pacientes. Estos problemas de generalización deterioran el rendimiento del sistema y se acentúan ante cambios en el dominio, como por ejemplo cambios en el procedimiento con el que se obtienen datos en los centros médicos \cite{Gu2023NaviPath}.

Gu et al. \cite{Gu2021Lessons} observan, además, que los datos patológicos del día a día están desbalanceados por naturaleza. Las áreas de metástasis a menudo ocupan porciones muy pequeñas de toda el corte WSI. Debido a esto, habría más anotaciones negativas que positivas. Este desbalance genera sesgos en las predicciones del modelo. Lian y Meng \cite{Liang&Meng2023} observan un ejemplo concreto de datos desbalanceados en el set BreaKHis \cite{BreaKHisDataset}, el cual contiene una cantidad significativamente mayor de muestras malignas que benignas. Ante este problema, Civit-Masot et al. \cite{CivitMasot2024} utilizan técnicas de aumento de datos como preprocesamiento para mejorar la distribución de muestras por categoría.

Sauter et al. \cite{Sauter2022} mencionan una serie de factores relacionados con los datos de entrenamiento que limitan la precisión en los algoritmos de clasificación. Algunos de estos son los desbalances de cantidad de muestras por clase, mal etiquetado, cantidad de datos disponibles y sesgos latentes en las muestras. En particular, los sesgos de medición, de muestreo y de correlación de clase, entre otros, con frecuencia ocasionan límites teórico-prácticos a la calidad de las predicciones del modelo.

Respecto a la clasificación de datos, Bellantuono et al. \cite{Bellantuono2023} proponen un modelo explicable para el diagnóstico de cáncer de tiroides y exponen sus limitaciones en casos con características anómalas. Encuentran casos donde un análisis histológico arroja que el tejido está sano, mientras que una prueba inmunohistoquímica revela la presencia de mutaciones que resultan en picos correspondientes a carotenoides. Los autores afirman que no existe un consenso médico sobre este respecto, puesto que no se acepta universalmente que los tejidos de estas características progresen a cáncer. Esto implica ambigüedad y posibles errores en la asignación formal de tales muestras histológicas.

Kiehl et al. \cite{Kiehl2022} objetan que un problema inmediato en los sistemas de IA es la carencia de datos suficientes de entrenamiento que representen la extensa variabilidad existente en las clínicas; es decir, la variabilidad inherente entre cada institución, entre regiones, diferentes etnias, entre otros factores. Contar con estos conjuntos de datos es un prerrequisito para la patología computacional de grado clínico. Los autores marcan la diferencia entre el grado clínico y el grado de investigación, siendo la primera más restrictiva.

Kiehl et al. observan que la baja disponibilidad de datos de entrenamiento etiquetados por patólogos también es un problema, aunque esto se puede mejorar adoptando métodos automatizados que vuelvan más eficiente el proceso de anotación \cite{Kiehl2022}.

Ullah et al. \cite{Ullah2024} advierten que hay ciertas preocupaciones en cuanto a combinar herramientas de diagnóstico asistido por computadora (CAD) con modelos de lenguaje extenso (LLM). Los LLMs pueden generar respuestas coherentes, pero carecen de verdadero entendimiento acerca de conceptos médicos. Las respuestas de este tipo de modelos están basadas en patrones estadísticos aprendidos durante el entrenamiento, lo cual podría no abarcar diagnósticos médicos complejos o intrincados. Por otro lado, el rendimiento de los LLMs es altamente dependiente de la calidad y la diversidad de los datos de entrenamiento. La presencia de sesgos e imprecisiones pueden pasar inadvertidas durante el entrenamiento y deteriorar el modelo, perpetuando resultados sesgados y dispares.

Bouderhem et al. \cite{Bouderhem2024} sostienen que las investigaciones de IA pueden verse restringidas debido a que los datos podrían no ser precisos o contener errores o diagnósticos equivocados. Los desarrolladores, en este caso, deben diseñar algoritmos que tomen en consideración una amplia gama de situaciones para todos los grupos de la población de datos. Los autores declaran que un algoritmo de IA sesgado inherentemente causa discriminación y predicciones erradas.

Otro requisito crucial para los desarrolladores es implementar indicadores de rendimiento que permitan medir el éxito de la IA,  que permitan a los médicos detectar posibles errores y sesgos. No considerar este factor puede desembocar en mala praxis \cite{Bouderhem2024}.

Holzinger et al. \cite{Holzinger2019} declaran que el principal problema de los sistemas de aprendizaje automático radica en que trabajan en forma estadística, lo que conlleva serias limitaciones en su desempeño. Tales sistemas no son capaces de comprender el contexto y, por ende, no pueden razonar sobre intervenciones y retrospecciones. Los autores sostienen que este problema se puede mitigar con la dirección de un modelo humano, similar a otros utilizados en investigaciones de causalidad, que permiten identificar por qué se arriba a un resultado concreto.
