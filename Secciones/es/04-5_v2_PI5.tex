%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   PI5 - ANÁLISIS DE RESULTADOS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{PI5 - ¿Qué aspectos se declaran pendientes de investigación en los artículos analizados?} \label{section:es/resultados-pi5}

Finzel et al. \cite{Finzel2022} observan una carencia de explicadores de redes gráficas que generen reportes verbales similares al lenguaje natural. La gran mayoría de explicadores buscan la subestructura con mayor relevancia según pérdidas, métricas y suposiciones de diseño.

Yin et al. \cite{Yin2020} sostienen que para alcanzar una integración eficaz con los patólogos en la toma de decisiones diarias, además de lograr sistemas interpretables se debería tomar en cuenta la causabilidad, concepto propuesto por Holzinger et al. \cite{Holzinger2019}, la cual se mide en términos de efectividad, eficiencia, satisfacción relacionada con la comprensión causal y su transparencia para un usuario. Tratándose de cuestiones de calidad de la explicación, incorporar este factor permite a un patólogo experto analizar la causalidad de una enfermedad concreta. Yin et al. declaran además que, si bien su modelo propuesto es interpretable, incorporar causabilidad es una meta futura que hará que su sistema sea útil y apto para el uso de los patólogos.

Sabol et al. \cite{Sabol2020} observan que su clasificador explicable de cáncer colorrectal debe someterse a pruebas clínicas. Destacan que en la práctica médica los datos suelen estar desbalanceados, ser heterogéneos e incluso inexactos, y declaran que sus próximos pasos son evaluar cómo responde el modelo con datos imperfectos.

Shawi et al. \cite{Shawi2022} observan varios puntos de mejora para su clasificador interpretable de cáncer de mama. Sugieren, por un lado, que un posible próximo paso es extender su trabajo y considerar técnicas diversas de interpretabilidad que extraigan conceptos relevantes de un conjunto de datos. Por otro lado, otra posible dirección es expandir la aplicación de su propuesta a otros problemas de la histopatología y desarrollar herramientas que permitan a los patólogos a diagnosticar más rápido y mejor.

Roscher et al. \cite{Roscher2020} advierten que la gran mayoría de publicaciones que usan aprendizaje automático no se ocupa de aspectos de interpretabilidad o explicabilidad. Los autores manifiestan interés en que la encuesta que llevaron a cabo proporcione ideas y metodologías acerca de cómo extraer información relevante sobre un objeto de estudio puntual. Sostienen además que la inferencia causal puede desempeñar un rol crucial en explicabilidad, pero requiere más investigación para adoptar su uso.

Nazar et al. \cite{Nazar2021} observan que, dado que IAX es un campo de investigación relativamente reciente, aún quedan desafíos pendientes por resolver en el ámbito médico. Las dificultades observadas en su revisión incluyen mejorar y agregar nuevos
enfoques de explicabilidad en ciertos dominios, y aumentar la interpretabilidad y la precisión del modelo. Los autores sostienen que se le debe dar prioridad a estos puntos en el futuro.

Palatnik de Sousa et al. \cite{PalatnikdeSousa2019} declaran pendiente realizar estudios futuros con la colaboración de patólogos expertos, centrados en evaluar más en profundidad las explicaciones de IA y aplicar su propuesta agnóstica del modelo a otros conjuntos de imágenes de diferentes dominios médicos.

Tschandl et al. \cite{Tschandl2020} concluyen que las herramientas de refinamiento centradas en las personas mejoran la experiencia de usuario de CBIR (del inglés content-based image retrieval) en patología. Fundamentados en esto, sostienen que deberían estudiarse más variedad de diseños y combinaciones de colaboración humano-IA y que está pendiente el evaluar su impacto en la prestación de servicios de salud.
 
Vanitha et al. \cite{Vanitha2024} declaran que su arquitectura propuesta, basada en la combinación de MobileNet, Xception y Grad-CAM, es prometedora. Sugiere como futuras investigaciones explorar combinar este marco de trabajo a otros tipos de cáncer o a tareas de diagnóstico más complejas para incrementar su rango de aplicaciones. La búsqueda de estos avances podría allanar el camino para obtener procesos más automatizados, precisos y confiables, que mejoren el bienestar de los pacientes en oncología.

Dörrich et al. \cite{Doerrich2023} declaran que el futuro explorarán una estrategia alternativa para anotar los núcleos celulares con un mayor número de clases distintas. Las investigaciones futuras también pueden contribuir a identificar otras características útiles para reconocer subtipos de cáncer. Los autores sugieren, además, que se podrían aplicar técnicas adicionales de IA explicable para estudiar a fondo las características discriminantes de cada clase.

Según Klauschen et al. \cite{Klauschen2024} es necesario tomar acciones con respecto a la privacidad de los datos, tanto durante el entrenamiento como el despliegue del modelo. Destacan que el aprendizaje federado se ha mostrado prometedor en este sentido.

Basaad et al. \cite{Basaad2024} postulan que su sistema propuesto es un paso inicial en la detección de cáncer de mama metástico y que es preciso hacer validaciones con datos clínicos. Sostienen que las investigaciones futuras que puedan realizarse serán esenciales para evaluar la respuesta del sistema en poblaciones diversas de pacientes y su integración a la práctica clínica.

Palkar et al. \cite{Palkar2024} identifican puntos de mejora y posibles investigaciones futuras relacionadas a su modelo interpretable pronosticador de glioma. Por un lado, sostienen que se puede buscar colaboración internacional para combinar datos de distintos países y crear una iniciativa global de investigación de glioma. Implementar un sistema en la nube puede favorecer esta iniciativa y aumentar la escalabilidad. Los autores añaden que se llevarán a cabo validaciones rigurosas con pruebas clínicas y que los planes de estudio adecuados reducirán la brecha entre informáticos y médicos expertos. Por último, destacan que las consideraciones éticas seguirán siendo primordiales, tanto en sentido de la privacidad del paciente como de la seguridad de los datos.

Civit-Masot et al. \cite{CivitMasot2024} observan que es pertinente consolidar los resultados de su modelo de clasificación de cáncer cervical usando un volumen de datos mayor, con imágenes de diferentes hospitales, para evitar el uso de técnicas de aumento de datos. Por otro lado, gracias a los tiempos de ejecución alcanzados, los autores sostienen que su propuesta da pie a una posible rama de investigación como continuación de su trabajo, cuyo enfoque se base en integrar el clasificador en sistemas embebidos.

Sabol et al. \cite{Sabol2019} observan que en futuras investigaciones se concentrarán en evaluar cómo la semántica de la información afecta al proceso de toma de decisión de los doctores utilizando diferentes métodos de aprendizaje automático interpretable. 

Finzel et al. \cite{Finzel2024} comentan que un desafío pendiente es desarrollar un marco de trabajo de explicaciones integrales, que sea dinámico, y que permita a los usuarios cambiar entre explicaciones locales o globales, seleccionar diferentes modalidades de explicación acorde al dominio, entre otras características. Observan además una brecha entre la investigación basada en aplicaciones y las centradas en el humano (human-centered) y añaden que falta evidencia empírica que demuestre la utilidad real de los modelos explicativos, tanto para expertos como novatos. Los autores creen que proporcionar marcos de trabajo centrados en las personas allanará el camino para integrar estas tecnologías y fomentará la confianza y comprensión de los usuarios.

Pahud de Mortanges et al. \cite{PahuddeMortanges2024} destacan que aún existen desafíos no resueltos que impiden aprovechar al máximo el potencial de IAX. Algunos de ellos incluyen la falta de estudios que traten sobre enriquecer sistemas IAX con otros tipos de datos clínicos, lo que se conoce como IAX multimodal, o usar conjuntos de datos longitudinales. Postulan que el desarrollo de IAX multimodal y longitudinal es crucial y necesario en muchos flujos de trabajo clínicos.

Respecto a su orquestador IAX propuesto, Pahud de Mortanges et al. \cite{PahuddeMortanges2024} destacan que, debido a las responsabilidades atribuidas a dicho sistema en la coordinación de sistemas IA/IAX específicos, aún es necesario abordar varios desafíos para garantizar su confiabilidad y la seguridad de los datos.

Cai et al. \cite{Cai2019} destacan que existen claras oportunidades para diseñar y probar recursos basados en los hallazgos de su estudio, por ejemplo acerca de cómo los asistentes de IA pueden dar forma a las prácticas laborales o ayudar a las personas a desarrollar estrategias más precisas, cómo inculcar modelos mentales más prácticos, o cuáles son los efectos en la confianza del usuario. Advierten, sin embargo, que su estudio se focalizó en recolectar las impresiones iniciales de los usuarios, y que la relación de uno con la herramienta puede evolucionar con el tiempo.

Gu et al. \cite{Gu2023XPath} comentan que su propuesta xPath tiene puntos de mejora, y observan que los trabajos futuros deberían considerar usar imágenes de múltiples centros de salud, convocar participantes con experiencias variadas, y conducir comparaciones con estudios a largo plazo, constatando el desempeño de xPath con el microscopio óptico. Los autores sostienen que recopilar toda esta información les permitirá validar el desempeño de su propuesta de una forma más integral.

%Por último, a través de los hallazgos de Gu et al. \cite{Gu2021Lessons} y Gu et al. \cite{Gu2023XPath}, Gu et al. 
Por último, a través de los hallazgos presentados en \cite{Gu2021Lessons} y \cite{Gu2023XPath}, Gu et al. \cite{Gu2023NaviPath} observan que, si bien los doctores prefieren los diseños simples, estos generalmente acarrean una pérdida en la comprensión de información y podrían no ser suficientemente informativos para su flujo de trabajo y la toma de decisiones. Los autores sugieren que las próximas investigaciones de HCI estudien qué información se debe conservar y qué se debe descartar mediante estudios empíricos.