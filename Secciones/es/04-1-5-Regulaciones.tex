%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   PI1 - ANÁLISIS DE RESULTADOS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Aspectos regulatorios}

%In relation to regulations, Tosun et al. \cite{Tosun2020} reveal that a proposed regulation before the European Union would prohibit “automatic processing” unless people are safeguarded. They foresee that future laws may further restrict AI use in professional practices, which represents a huge challenge to industry.
En términos regulatorios, Tosun et al. \cite{Tosun2020} revelan que una regulación propuesta ante la Unión Europea podría prohibir el procesamiento automático a menos que se tenga en cuenta la protección de los datos personales. Prevén además que en un futuro se implementen leyes que restrinjan aún más el uso de IA en prácticas profesionales.

%Dos-Santos et al. \cite{dosSantos2022} affirm that AI algorithms must work under regulatory standards for testing and usage in medical facilities. According to Tosun et al. \cite{Tosun2020}, this is already a known necessity in radiology.
Dos-Santos et al. \cite{dosSantos2022} afirman que los algoritmos de IA deben funcionar regidos por estándares regulatorios para pruebas y uso en instalaciones médicas. De acuerdo con Tosun et al. \cite{Tosun2020} esto es una necesidad conocida en radiología.

%Thus, due to data privacy, Rösler et al. \cite{Roesler2023} observe that professionals must work under an ethical approval and informed consent of the patient prior to the use and evaluation of patient-related data. In addition to this, if possible, anonymized raw data have to be included from the start of the training process.
Respecto a la privacidad de la información, Rösler et al. \cite{Roesler2023} observan que los profesionales deben contar con el consentimiento informado del paciente antes de hacer uso de sus datos.

%A escribir
Zhou et al. \cite{Zhou2021} observan que la falta de estándares en los protocolos de adquisición de datos, en términos del equipamiento y la configuración utilizados, provoca deriva en el desempeño del sistema. Destacan también que se carece de estándares que establezcan la forma de etiquetar las imágenes médicas, susceptibles de ser utilizadas para entrenar un modelo.

Hauser et al. \cite{Hauser2022} mencionan que la Unión Europea apela a la transparencia de los modelos de inteligencia artificial como un requisito legal. No obstante esto, mencionan que en la literatura médica aún está vigente el debate respecto a la necesidad y la utilidad de implementar IAX.

Por su parte, Tjoa y Guan \cite{Tjoa&Guan2021} observan que puede ser necesario alejar el estudio de la interpretabilidad de los estudios centrados en algoritmos. Objetan, además, que el establecimiento de requisitos estándar en el desarrollo de modelos podría frenar los procesos de investigación propiamente dichos, pero consideran que puede ser una manera eficiente de concertar acuerdos que velen por evitar daños.

Nazar et al. \cite{Nazar2021} observan que la naturaleza de caja negra de los algoritmos de aprendizaje profundo puede acarrear cuestiones legales y éticas que reduzcan la confianza de los usuarios, principalmente por el uso de información sensible de naturaleza médica.

Wenzel y Wiegand \cite{Wenzel&Wiegand2020} comentan que la definición de estándares en sistemas de inteligencia artificial dentro del área médica es compleja debido a los múltiples casos de aplicación, que van desde detectar nódulos en muestras pulmonares, incluyendo interpretación de datos de pacientes, hasta sistemas basados en diálogo que formulan preguntas sistemáticas sobre síntomas y sugieren procedimientos. Tomando este contexto como punto de partida, los autores encuentran que diversos organismos comenzaron a trabajar en la definición de estándares, entre los que se encuentran: el plan federal del U.S. National Institute of Standards and Technology (NIST) en el año 2019; un consorcio dirigido por el Chinese Electronics Standard Institute en 2018; la publicación “Ethics Guidelines for Trustworthy Artificial Intelligence” por parte de la Unión Europea en 2019; y los grupos de trabajo del Deutches Institut für Normung (DIN) en el año 2019.

Según Ullah et al. \cite{Ullah2024}, la integración de LLMs en el diagnóstico médico conlleva cuestiones éticas a considerar. Es preciso priorizar la seguridad y privacidad de los datos sensibles de los pacientes para evitar ocasionarles daños de ningún tipo y las implicancias legales que pueda acarrear. Los autores postulan que los organismos regulatorios y las sociedades profesionales deberían colaborar para establecer estándares y marcos de trabajo que aseguren implementaciones de aplicaciones basadas en LLM que sean éticas y seguras.

Bouderhem et al. \cite{Bouderhem2024} advierten que el marco de trabajo regulatorio actual podría no ser suficiente o apropiado para la naturaleza de la IA. Es necesario contar con más colaboración y diálogo entre las partes interesadas (gobiernos, entes reguladores, desarrolladores, profesionales de la salud y pacientes) para establecer estándares claros y lineamientos para el uso de IA en el ámbito médico. Citan el uso de ChatGPT y la privacidad de los datos como dos escenarios donde la carencia de regulación es notable.