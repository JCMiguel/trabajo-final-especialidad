%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   PI1 - ANÁLISIS DE RESULTADOS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Transparencia del proceso}

%Explainability is considered one of the prerequisites for AI in medicine. In this sense, xAI is seen as a step towards the realization of the FATE principles in AI \cite{ShabanNejad2021}.
Se considera a la explicabilidad como uno de los prerrequisitos del uso de IA en medicina. En este sentido, IAX es visto como un paso hacia la realización de los principios FATE en IA \cite{ShabanNejad2021}.

%Holzinger et al. \cite{Holzinger2019} propose that it is necessary to understand the causality of representations. This is why they differentiate the words “explainability” and “causality”. Explainability highlights decision-relevant parts of the used representations of the algorithms and active parts in the algorithmic model and it does not refer to an explicit human model. Causability is defined as the extent to which an explanation of a statement to a human expert achieves a specified level of causal understanding in a specified context of use. Establishing causability as a solid scientific field becomes imperative in this sense.
Holzinger et al. \cite{Holzinger2019} proponen que es necesario entender la causalidad de los resultados, diferenciando entre explicabilidad y causalidad. La explicabilidad destaca partes relevantes para la decisión de las representaciones utilizadas de los algoritmos y no se refiere a un modelo humano explícito. Por su parte, la causalidad se define como el grado en que la explicación de una afirmación logra un nivel específico de comprensión causal para un experto en un contexto específico. En este sentido, se vuelve imperativo establecer la causalidad como un campo científico sólido.

%A escribir
Teng et al. \cite{Teng2022} observan que una gran parte de las técnicas de interpretabilidad propuestas hasta el momento están diseñadas como marcos de trabajo genéricos y que no son específicos para tratar con campos sensibles del dominio médico. Si bien los modelos interpretables de aprendizaje profundo han tenido buenos resultados, destacan que aún persiste la carencia de conceptos estándar o métricas de evaluación definidas. Fundamentan su postura basado en que generalmente la interpretabilidad se evalúa en forma intuitiva observando imágenes, pero en el ámbito médico este hecho podría acarrear un impacto negativo. Por lo tanto, para lograr modelos más confiables y robustos es necesario contar con métodos cuantitativos de análisis de intepretabilidad que permitan evaluar su efectividad real en la práctica clínica.

Sabol et al. \cite{Sabol2020} sostienen que los modelos predictivos deberían ser precisos y responsables; es decir, que deberían declarar la incertidumbre en sus predicciones e indicar que para casos complejos sea necesario constatar con la inspección de un experto. Es por eso que otro enfoque de interpretabilidad en los algoritmos de aprendizaje automático es medir la incertidumbre predictiva, es decir, cuán incierta es la predicción para un ejemplo particular de muestra.

Respecto a los modelos interpretables mediante técnicas post-hoc (LRP, Grad-CAM, entre otros), Schuhmacher et al. \cite{Schuhmacher2022} observan que, aunque muestran una salida interpretable, los enfoques existentes carecen de una definición clara de lo que constituye una interpretación o explicación válida. Destacan además que, pese al esfuerzo de varios autores por proponer definiciones de interpretabilidad y explicabilidad, aún no se logró un consenso uniforme. Por su parte, Nazar et al. \cite{Nazar2021} arriban a conclusiones similares.

Gallo et al. \cite{Gallo2023} concluyen que es complejo evaluar la calidad de las explicaciones generadas por un modelo IA/IAX. Observan que una buena explicación debería cumplir varios requisitos, como por ejemplo ser interpretable y fiel al modelo. Debido a la subjetividad subyacente en las explicaciones, a menudo se consulta con expertos del dominio para evaluar su efectividad a través de cuestionarios. No obstante, en este sentido la calidad se ve afectada por la experiencia de los usuarios. Los autores advierten además que la forma en que se presentan los resultados del sistema puede afectar la utilidad y confiabilidad percibida del mismo.

Pese a estas dificultades, Gallo et al. mencionan que existen métodos automáticos para medir la solidez de una explicación en un sistema de inteligencia artificial. Algunos de ellos se basan en insertar perturbaciones y evaluar la degradación de su desempeño, ya sea a través de la Sensitivity-n, el área sobre la curva perturbada (AOPC), entre otros métodos.

Nasir et al. \cite{Nasir2024} destacan que la falta de interpretabilidad en modelos de inteligencia artificial obstaculiza la identificación y mitigación de sesgos. Dichos sesgos pueden ocasionar graves consecuencias en el ámbito médico: un diagnóstico erróneo, tratamiento deficiente y potencial daño a la salud de los pacientes. La inexplicabilidad y falta de transparencia de los sistemas de IA también provocan dilemas éticos y deterioran su confiabilidad. Los autores citan un caso de estudio de aplicabilidad de GPT-4 como un chatbot médico en el que se concluye que, si bien tiene beneficios al reducir la sobrecarga laboral de los médicos y proveer un servicio de atención las 24 horas a los pacientes, acarrea también serias limitaciones, como la incapacidad de comprender el contexto completo, la falta de criterio clínico y la dependencia con los datos de entrenamiento. Esto involucra múltiples riesgos de aspecto ético y legal, razón por la cual debería implementarse cautelosamente con el fin de complementar al experto, en lugar de reemplazarlo.

Ullah et al. \cite{Ullah2024} advierten que los modelos de lenguaje extenso (LLMs) tienen naturaleza de caja negra. Por lo tanto carecen de explicabilidad y acarrean los mismos problemas y dificultades que otros algoritmos de inteligencia artificial. El modelo no provee razones transparentes para sus decisiones. Esto dificulta que los profesionales del ámbito médico validen y confíen sus recomendaciones, e invalida que estos sistemas puedan utilizarse en escenario donde la toma de decisiones es crítica. Los autores sostienen que los avances recientes en algoritmos evolutivos y genéticos, en términos de verificación de rendimiento, ofrecen posibles soluciones a esta preocupación.

Finzel et al. \cite{Finzel2024} observan que los modelos de aprendizaje profundo interpretables pueden no ser explicables debido a las relaciones complejas entre conceptos y jerarquías conceptuales. A su vez, proponen que estructurar las razones de una determinada clasificación a través de una interacción basada en el diálogo entre el usuario y el sistema podría ayudar a navegar las diferentes rutas de decisión y estrategias del modelo.

Bouderhem et al. \cite{Bouderhem2024} declaran que la explicabilidad es un gran desafío. Los sistemas de IA son criticados por su opacidad dado que los observadores e investigadores no conocen cómo alcanzan ciertas decisiones. La confianza pública en salud digital podría verse amenazada si las personas no confían en sus servicios. Es crucial comprender claramente cómo funcionan los sistemas de IA. Para esto, es preciso determinar sus limitaciones para que los pacientes puedan tomar decisiones y den su consentimiento informado, incluso en emergencias médicas. Los autores sostienen que proporcionar métodos que vuelvan explicable a la IA aportará transparencia y confiabilidad.
