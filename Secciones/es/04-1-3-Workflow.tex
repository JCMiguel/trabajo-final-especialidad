%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%   PI1 - ANÁLISIS DE RESULTADOS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Acoplamiento al flujo de trabajo}

%Ayorinde et al. \cite{Ayorinde2022} postulate that it is not necessary to achieve a perfect AI system for it to be used in the hospitals. In fact, it is more probable that the most useful tools are a combination between AI models and a set of work rules that favor the effective human supervision.
Ayorinde et al. \cite{Ayorinde2022} postulan que no es necesario lograr un sistema IA perfecto para que pueda ser utilizado en las clínicas. Sostienen, en cambio, que es probable que las herramientas más útiles sean una combinación entre modelos de IA y conjuntos de reglas de trabajo que favorezcan una supervisión humana efectiva.

%According to Tosun et al. \cite{Tosun2020} the main function of xAI in pathology is to promote safety, reliability, and accountability in addressing issues with bias, transparency, safety, and causality. The authors find, however, that there is not yet a consensus on how pathologists should supervise or work with computational pathology systems. As patient safety in pathology is the result of a complex interaction between pathologists, other physicians, laboratory personnel and computational pathology applications, they conclude that xAI must help the pathologist be more precise and efficient in their work.
Según Tosun et al. \cite{Tosun2020}, la principal función de IAX en patología es promover la seguridad y confiabilidad al abordar cuestiones con sesgos, transparencia y causalidad. Los autores observan, no obstante, que no hay un consenso respecto a cómo deberían supervisar los patólogos los sistemas de patología computacional, o sobre cómo trabajar con ellos. En estos términos, el bienestar del paciente es el resultado de una interacción compleja entre patólogos, médicos, personal de laboratorio y las aplicaciones computacionales. Por esta razón, los autores concluyen que las técnicas IAX deben asistir a los patólogos a que sean más precisos y eficientes en su trabajo.

%Jaharri et al. \cite{Jarrahi2022} claim that the IA/IAX systems employed in medicine present interoperability challenges, given that these systems often show an amount of information or in formats that appear indecipherable to physicians. These authors offer that an expert-in-the-loop AI work system could clarify a mutual workflow between humans and machines.
Jarrahi et al. \cite{Jarrahi2022} postulan que los sistemas IA/IAX utilizados en medicina poseen problemas de interoperabilidad, dado que a menudo presentan demasiada información en formatos indescifrables para los médicos. Los autores de este estudio proponen que un sistema de IA intervenido por expertos, también llamado \enquote{experto en el bucle} o \enquote{expert-in-the-loop}, podría clarificar un flujo de trabajo mutuo entre personas y computadoras.

%As for Verma et al. \cite{Verma2023}, they suggest that AI can be included in collective decision-making processes in oncology either as a tool or as a member, each of these alternatives generating different sets of ethical, societal and technological issues. In an interview they conducted for their article \cite{Verma2023} with seven physicians working at the Lausanne University Hospital (CHUV), in relation to AI systems autonomy, they detected a consensus on how trust cannot be put in something that is not trustful or bypasses the doctors. One of the experts that took part in this interview added that an AI model cannot be trusted if it is not able to choose which treatment modality works best for a particular patient.
Por su parte, Verma et al. \cite{Verma2023} sugieren que la inteligencia artificial puede incluirse en procesos colectivos de toma de decisiones, ya sea como miembro o como herramienta. Realizaron una entrevista a siete médicos del Lausanne University Hospital (CHUV) en la que se les preguntó cuestiones éticas, sociales y tecnológicas sobre la inteligencia artificial. Respecto a la autonomía de los sistemas IA, los autores observan un consenso en que no se puede recurrir a algo que no es confiable e intentar sobrepasar a los doctores. Uno de los expertos entrevistados añade además que no se puede confiar en un sistema si este no es capaz de elegir la modalidad más adecuada de tratamiento para un paciente en concreto.

Nazar et al. \cite{Nazar2021} observan que la aplicabilidad de IAX en diferentes dominios aún requiere esfuerzos de investigación y estudiarse en profundidad. Por otro lado, concluyen que es preciso obtener explicaciones a las decisiones de los sistemas de IA, y que el usuario final debería poder comprenderlas.

Gu et al. \cite{Gu2021Lessons} encuentran que para el sistema es complejo detectar lesiones pequeñas de tejido en cortes WSI, y advierten que, según consultas con profesionales, es mucho más valiosos encontrar estas áreas pequeñas con inteligencia artificial, ya que las lesiones más grandes se pueden localizar rápidamente sin asistencia.

Por otro lado, Gu et al. \cite{Gu2023XPath} comentan que es difícil convencer a los patólogos sobre transformar el diagnóstico manual en prácticas asistidas por inteligencia artificial. Los autores creen que la causa de esto subyace en que, a pesar de que se investigan formas de mejorar el desempeño de los sistemas de IA, hay una notoria carencia de entendimiento acerca de cómo los doctores se podrían beneficiar de la inteligencia artificial y usarla en diagnósticos. Y si bien las investigaciones sobre IAX tienen por objetivo principal explicar los hallazgos de los modelos, más que optimizar su rendimiento, argumentan que no es suficiente para asistir a los patólogos contar con algoritmos optimizados o aplicar IAX. Una integración deficiente en el flujo de trabajo médico podría suponer una carga mayor para los patólogos, factor que desincentiva el uso de sistemas de inteligencia artificial en la práctica.

Dolezal et al. \cite{Dolezal2024} observan que es crucial que los sistemas de aprendizaje profundo sean transparentes e interpretables para poder asistir de manera efectiva a la toma de decisiones clínicas, y afirman que el software que integra explicabilidad con cuantificación de la incertidumbre presenta una ventaja notable para ser adoptada como herramienta clínica.

Kiehl et al. \cite{Kiehl2022} destacan que adoptar patología computacional acarrea riesgos de sesgos por automatización, en el que el usuario puede confiar ciegamente en el resultado del sistema, sin hacer validaciones suficientes sobre la calidad del mismo. Este riesgo también es percibido por Ullah et al. \cite{Ullah2024}. Estos últimos observan que la incorporación de LLMs en diagnósticos médicos puede deteriorar la autonomía de los profesionales y poner en riesgo el pensamiento crítico y el razonamiento clínico independiente. Ullah et al. advierten también que, aunque ChatGPT se muestra prometedor en varias aplicaciones, es necesario aplicar validaciones rigurosas, estudios a largo plazo y pruebas en la práctica cotidiana para evaluar su desempeño, confiabilidad e impacto en el resultado de los pacientes. 

Finzel et al. \cite{Finzel2024} analizan que los métodos basados en resaltar áreas relevantes de píxeles en las imágenes de entrada, aplicados a redes convolucionales, pueden ser interpretables por expertos en el dominio, pero el significado de este resaltado podría ser ambiguo y fuertemente dependiente de si la etiqueta de verdad fundamental coincide con el resultado de la red neuronal. Añaden que es necesario brindar algún significado a las áreas de píxeles importantes, si los destinatarios de la explicación son novatos, y mencionan investigaciones recientes de explicaciones basadas en conceptos.

Bouderhem et al. \cite{Bouderhem2024} declaran que es necesario ver más allá de la publicidad exagerada del uso de la IA y evaluar las ventajas y desventajas de su uso en el ámbito sanitario. La inteligencia artificial en medicina posee nuevos desafíos que es preciso afrontar, como la presencia de sesgos o la responsabilidad subyacente en situaciones en que los reportes médicos de los pacientes fueran vulnerados o robados.

Respecto a los sistemas de asistencia de decisiones clínicas (CDSS, del inglés clinical decision support system), Cai et al. \cite{Cai2019} destacan que algunos trabajos demuestran que pueden ser difíciles de implementar con éxito en la práctica, citando como causa raíz la falta de consideración de lineamientos de Interacción Humano-Computadora (HCI, de Human computer interaction). Los usuarios pueden resistirse a adoptar una herramienta si no comprenden sus capacidades o su utilidad a comparación de las prácticas preexistentes. A este factor, se añade también la aversión algorítmica como desafío subyacente a esta clase de sistemas.

Cai et al. también encuentran que algunos autores descubrieron que las prácticas existentes favorecen la socialización entre médicos, y si la tecnología de diagnóstico asistido carece de esta capacidad de razonar decisiones, puede ser difícil integrarla al esquema de trabajo actual.

Klauschen et al. \cite{Klauschen2024} advierten que existe un obstáculo interdisciplinario, puesto que los expertos en aprendizaje automático y los patólogos necesitan aprender a interactuar entre sí y encontrar un lenguaje común. Observan que la currícula académica actual raras veces incorporan los cursos necesarios para que los estudiantes puedan cooperar con miembros de otras disciplinas. Los autores sugieren además que es necesario contar con planes de estudio académicos que sean novedosos, en niveles que van desde especializaciones menores hasta educación de posgrado.
